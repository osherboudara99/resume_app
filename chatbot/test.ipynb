{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 2.18G/2.18G [00:42<00:00, 51.1MiB/s]\n",
      "Verifying: 100%|██████████| 2.18G/2.18G [00:03<00:00, 714MiB/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Phi-3-mini-4k-instruct.Q4_0.gguf\"\n",
    "\n",
    "model = GPT4All(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The product of 500 multiplied by 500 is:\n",
      "\n",
      "500 * 500 = 250,000.\n"
     ]
    }
   ],
   "source": [
    "with model.chat_session():\n",
    "    print(model.generate(\"What is 500 * 500\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'order': 'q',\n",
       " 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5',\n",
       " 'name': 'Phi-3 Mini Instruct',\n",
       " 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf',\n",
       " 'filesize': '2176181568',\n",
       " 'requires': '2.7.1',\n",
       " 'ramrequired': '4',\n",
       " 'parameters': '4 billion',\n",
       " 'quant': 'q4_0',\n",
       " 'type': 'Phi-3',\n",
       " 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href=\"https://opensource.org/license/mit\">MIT</a></li><li>No restrictions on commercial use</li></ul>',\n",
       " 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf',\n",
       " 'promptTemplate': '<|user|>\\n{0}<|end|>\\n<|assistant|>\\n{1}<|end|>\\n',\n",
       " 'systemPrompt': '',\n",
       " 'chatTemplate': \"{{- bos_token }}\\n{%- for message in messages %}\\n    {{- '<|' + message['role'] + '|>\\\\n' + message['content'] + '<|end|>\\\\n' }}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|assistant|>\\\\n' }}\\n{%- else %}\\n    {{- eos_token }}\\n{%- endif %}\",\n",
       " 'path': 'C:\\\\Users\\\\Osher N Boudara\\\\.cache\\\\gpt4all\\\\Phi-3-mini-4k-instruct.Q4_0.gguf'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.retrieve_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
